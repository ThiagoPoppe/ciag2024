{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d726cc",
   "metadata": {},
   "source": [
    "# Sequências - Aula Prática 02/04\n",
    "## RNNs (Recurrent Neural Networks)\n",
    "\n",
    "Neste notebook iremos continuar nossos estudos de redes neurais recorrentes (RNNs), trabalhando dessa vez com geração de nomes a nível de caractere, utilizando os mesmos dados do notebook `name_classification.ipynb`.\n",
    "\n",
    "> Em mais detalhes, dado um *idioma*, queremos construir uma rede (*language model*) para gerar um *nome* que possui características dos nomes originários daquele idioma.\n",
    "\n",
    "- Esse notebook foi fortemente inspirado no segundo tutorial da série [NLP From Scratch](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html), disponibilizado no site do PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso esteja executando esse notebook no ambiente da Tatu, por favor execute a seguinte célula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variáveis de ambiente http_proxy e https_proxy configuradas!\n"
     ]
    }
   ],
   "source": [
    "%load_ext nbproxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nltk in /u/ej0f/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.5)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Precisaremos instalar o pacote NLTK para conseguir realizar algumas análises\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c310f",
   "metadata": {},
   "source": [
    "## Importação de pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2a1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795b6b30-6d1a-4b86-b15f-67e02560c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device escolhido: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verificando se temos CUDA disponível e selecionando o device que será utilizado\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device escolhido:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cefc1bd-91a1-4c01-a1df-8cb2c1ccc31a",
   "metadata": {},
   "source": [
    "## Processamento da base de dados\n",
    "\n",
    "Como mencionado no ínicio do notebook, iremos trabalhar com os mesmos dados vistos no notebook `name_classification.ipynb`. Porém, dessa vez, iremos trocar o que será entrada e saída do nosso modelo. No notebook anterior, tinhamos como entrada um nome (sequência) e como saída um idioma. Dessa vez, queremos admitir como entrada um idioma e *gerar* uma nome letra por letra.\n",
    "\n",
    "Durante esse notebook, iremos trabalhar com os dados localizados na pasta `/pgeoprj/ciag2023/datasets/sequence_datasets/names/`. Nela, você verá 18 arquivos `.txt` separados por países, onde cada arquivo contém uma lista de nomes originários daquele país.\n",
    "\n",
    "> Para esse notebook, iremos processar os dados de maneira similar, criando um dicionário que mapeia `linguagem -> lista de nomes`.\n",
    "\n",
    "<!-- e também definindo a função `unicode2ascii` para remover caracteres especiais de idiomas específicos. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cf3876-c2ee-46ec-8e7e-25d2407202e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/pgeoprj/ciag2023/datasets/sequence_datasets/names/Arabic.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Dutch.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/English.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/French.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/German.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Greek.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Irish.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Italian.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Japanese.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Korean.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Polish.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Portuguese.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Scottish.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Spanish.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Vietnamese.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Chinese.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Czech.txt',\n",
       " '/pgeoprj/ciag2023/datasets/sequence_datasets/names/Russian.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths = glob('/pgeoprj/ciag2023/datasets/sequence_datasets/names/*.txt')  # * aqui quer dizer \"qualquer string\"\n",
    "filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b08b11f-e3ff-4a75-9f06-c958a18ad791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiro filepath: /pgeoprj/ciag2023/datasets/sequence_datasets/names/Arabic.txt\n",
      "Linguagem do primeiro filepath: Arabic\n"
     ]
    }
   ],
   "source": [
    "def get_language(filepath):\n",
    "    filename = filepath.split('/')[-1]\n",
    "    language = filename.split('.')[0]\n",
    "\n",
    "    return language\n",
    "\n",
    "print('Primeiro filepath:', filepaths[0])\n",
    "print('Linguagem do primeiro filepath:', get_language(filepaths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8a8148e-7955-4599-b63f-c297e9ec2a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 primeiros nomes em árabe: [['K', 'h', 'o', 'u', 'r', 'y'], ['N', 'a', 'h', 'a', 's'], ['D', 'a', 'h', 'e', 'r'], ['G', 'e', 'r', 'g', 'e', 's'], ['N', 'a', 'z', 'a', 'r', 'i']]\n",
      "Idiomas disponíveis no conjunto de dados: ['Arabic', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Scottish', 'Spanish', 'Vietnamese', 'Chinese', 'Czech', 'Russian']\n"
     ]
    }
   ],
   "source": [
    "languages = [] \n",
    "language_names = defaultdict(list)  # defaultdict é igual à um dict normal, porém não precisamos verificar\n",
    "                                    # manualmente se uma entrada já foi inicializada, nesse caso com uma lista vazia\n",
    "\n",
    "for filepath in filepaths:\n",
    "    language = get_language(filepath)\n",
    "    languages.append(language)\n",
    "    \n",
    "    with open(filepath, 'r') as fp:\n",
    "        lines = fp.readlines()  # convertendo as linhas de um arquivo em uma lista de linhas\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()  # removendo \\n e espaços no começo e fim da string\n",
    "            language_names[language].append([*line])  # usamos * para \"descompactar\" string em caracteres\n",
    "\n",
    "print('5 primeiros nomes em árabe:', language_names['Arabic'][:5])\n",
    "print('Idiomas disponíveis no conjunto de dados:', languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba280ce7-abf7-4cb7-b3be-d9fa49f28952",
   "metadata": {},
   "source": [
    "Seguindo as ideias trabalhadas em notebooks passados, precisamos criar um mapeamento entre *tokens* e índices. No nosso caso, o *token* é uma letra do nosso alfabeto. Para isso, iremos construir nosso alfabeto (vocabulário), com base nas letras únicas de todos os nomes da base de dados, e, posteriormente, iremos criar 2 estruturas para realizar tal mapeamento: `token2index` e `index2token`.\n",
    "\n",
    "> Além dos *tokens* obtidos através do texto, é muito comum vermos também outros 3 *tokens* especiais:\n",
    "> - **\\<sos\\>**: abreviação para *start-of-sequence*, ou início de sentença.\n",
    "> - **\\<eos\\>**: abreviação para *end-of-sequence*, ou fim de sentença.\n",
    "> - **\\<pad\\>**: *token* especial destinado para indicar um valor de *pad* na nossa sequência.\n",
    "\n",
    "- Diferentemente do notebook anterior, os *tokens* de **\\<sos\\>** e **\\<eos\\>** serão úteis, uma vez que eles são utilizados em contextos de geração de sentenças. O token **\\<pad\\>** ainda não será utilizado, uma vez que ainda não trabalharemos com *batches* por enquanto.\n",
    "  - Na verdade, iremos utilizar apenas o *token* **\\<eos\\>** no nosso vocabulário, já que forneceremos a primeira letra do nome que queremos gerar para o nosso modelo. Ao utilizar o *token* **\\<sos\\>** no nosso contexto, a nossa rede terá que adivinhar às cegas o primeiro *token* do nosso nome. A utilização de ambos *tokens* de início e fim de sentença são mais vistos em abordagens *seq2seq*, onde produzimos um contexto da frase de entrada, auxiliando a rede a prever o *token* inicial a partir de **\\<sos\\>** com mais garantia de acerto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81434dc8-70a4-4f14-bca3-ddfa27ae1d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do alfabeto: 83\n",
      "5 tokens aleatórios do alfabeto: ['-', 'u', 'u', 'ì', 'ê']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "\n",
    "# Adicionando o token especial <eos>\n",
    "vocabulary.append('<eos>')\n",
    "\n",
    "for names in language_names.values():\n",
    "    for name in names:\n",
    "        for token in name:        \n",
    "            if token not in vocabulary:\n",
    "                vocabulary.append(token)\n",
    "\n",
    "print('Tamanho do alfabeto:', len(vocabulary))\n",
    "print('5 tokens aleatórios do alfabeto:', random.choices(vocabulary, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d38da7e-5302-4d5d-bf54-27531a9dfc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token de índice 42: p\n",
      "Índice do token \"<eos>\": 0\n"
     ]
    }
   ],
   "source": [
    "index2token = []\n",
    "token2index = {}\n",
    "\n",
    "for token_idx, token in enumerate(vocabulary):\n",
    "    index2token.append(token)\n",
    "    token2index[token] = token_idx\n",
    "\n",
    "print('Token de índice 42:', index2token[42])\n",
    "print('Índice do token \"<eos>\":', token2index['<eos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73199e9-979e-407f-9716-fec5315ea64a",
   "metadata": {},
   "source": [
    "Agora que temos as estruturas `index2token` e `token2index` bem definidas, conseguimos converter um nome em um tensor de índices numéricos através das funções `token2tensor` e `name2tensor`, como demonstrado a seguir.\n",
    "\n",
    "> **Observação 1:** Criaremos um tensor de tamanho $n \\times 1$, onde $n$ é o tamanho do nome e $1$ o tamanho do nosso *batch*. Para esse notebook, nós não iremos trabalhar com `batch_size > 1`, devido à complicações relacionadas com manipulação de sequências e *padding*. Deixaremos tais assuntos para serem abordados no notebook de `seq2seq`.\n",
    "\n",
    "- Optamos por trocar a ordem padrão das dimensões dos dados em PyTorch, ou seja, com o tamanho do *batch* no começo, para facilitar operações em sequências no futuro, como utilizar `len` para obter o tamanho da sequência e tornar a indexação mais fácil. Na verdade, especificamente para sequências, PyTorch nos dá a opção de colocar o tamanho do *batch* como primeira dimensão do nosso tensor ou na segunda, sendo a primeira o tamanho da nossa sequência. Veremos isso em mais detalhes quando trabalharmos com os modelos recorrentes implementados pelo PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15d18b1c-b76f-4032-b7a7-9c5c74a28644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor para o token J: tensor([50])\n",
      "Tensor para o nome Jonas: tensor([[50,  3, 25,  8,  9,  0]])\n"
     ]
    }
   ],
   "source": [
    "def token2tensor(token):\n",
    "    return torch.tensor([token2index[token]], dtype=torch.long)\n",
    "\n",
    "def name2tensor(name):\n",
    "    tensor = torch.zeros((1 + len(name), 1), dtype=torch.long)\n",
    "    \n",
    "    # Adicionando os tokens de fim de sentença\n",
    "    tensor[-1] = token2tensor('<eos>')\n",
    "    \n",
    "    for idx, token in enumerate(name):\n",
    "        tensor[idx] = token2tensor(token)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "name = 'Jonas'\n",
    "print(f'Tensor para o token J:', token2tensor('J'))\n",
    "print(f'Tensor para o nome {name}: {name2tensor(name).T}')  # transposição para fins de print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2462d5c-c8cc-4d34-821f-ece8d2921fbb",
   "metadata": {},
   "source": [
    "Para finalizar essa parte do notebook sobre dados, iremos criar uma função auxiliar `get_random_pair` para selecionar de forma aleatória um par `(idioma, nome)` da nossa base dados. Além disso, tal função irá retornar os tensores relacionados com cada componente do par.\n",
    "\n",
    "- Por ora não iremos nos preocupar com a separação entre conjuntos de treino, validação e teste. O objetivo desse notebook é de ensinar como trabalhamos do zero com geração de sequências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc1a934-7161-46b9-a70d-f55cf23a169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def language2tensor(language):\n",
    "    return torch.tensor([languages.index(language)], dtype=torch.long)\n",
    "\n",
    "def get_random_pair():\n",
    "    language = random.choice(languages)\n",
    "    name = random.choice(language_names[language])\n",
    "    name = ''.join(name)  # convertendo lista de caracteres em string\n",
    "\n",
    "    name_tensor = name2tensor(name)\n",
    "    language_tensor = language2tensor(language)\n",
    "\n",
    "    return name, language, name_tensor, language_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e2952b4-92b5-4498-964c-4fea70809a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par selecionado: ('Torrens', 'English')\n",
      "Tensores do par: (tensor([[24,  3,  5,  5, 11, 25,  9,  0]]), tensor([2]))\n"
     ]
    }
   ],
   "source": [
    "name, language, name_tensor, language_tensor = get_random_pair()\n",
    "\n",
    "print('Par selecionado:', (name, language))\n",
    "print('Tensores do par:', (name_tensor.T, language_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a537a31-45bf-4756-96ff-89ff322cc3d0",
   "metadata": {},
   "source": [
    "## Definição do nosso modelo\n",
    "\n",
    "Como já tratamos a implementação de uma RNN do zero no notebook anterior, iremos utilizar o módulo `nn.RNN` disponibilizado pelo PyTorch. Uma documentação mais extensiva desse módulo pode ser encontrada [aqui](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#rnn). Não iremos utilizar todos os parâmetros customizáveis do módulo, mas iremos explicar eles aqui brevemente.\n",
    "\n",
    "- **`input_size`**: o número de *features* da entrada, no nosso caso por exemplo o tamanho da dimensão de *embedding*.\n",
    "- **`hidden_size`**: o tamanho do estado oculto ($h$) que utilizaremos.\n",
    "- **`num_layers`**: número de camadas recorrentes que iremos \"empilhar\".\n",
    "- **`nonlinearity`**: a camada de ativação não linear utilizado ao longo da recorrência.\n",
    "- **`bias`**: valor *booleano* indicando se queremos ou não utilizar viés nas camadas.\n",
    "- **`batch_first`**: valor *booleano* que permite a gente indicar se queremos trabalhar com dados onde a primeira dimensão é o tamanho do *batch*, nesse caso `batch_first=True`, ou o tamanho da sequência.\n",
    "- **`dropout`**: introduz *dropout* em cada saída da camada recorrente (usado com `num_layers != 1`).\n",
    "- **`bidirectional`**: valor *booleano* que indica se queremos um modelo recorrente bidirecional.\n",
    "\n",
    "Logo abaixo temos um exemplo de utilização do módulo `nn.RNN`. Suponha que após aplicarmos o embedding, nós tenhamos a matriz `X`, cujas dimensões são: $S \\times 1 \\times E$, onde $S$ é o tamanho da sequência e $E$ o tamanho do embedding escolhido.\n",
    "\n",
    "> Para esse exemplo utilizamos: $S = 4$, $E = 2$, $h = 8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da632ad-a1d3-48e1-9ab8-232bd4f350fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do output final: torch.Size([1, 8])\n",
      "Tamanho do hidden final: torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 4\n",
    "embedding_size = 2\n",
    "X = torch.rand(seq_length, 1, embedding_size)  # definindo uma matriz X aleatória apenas para exemplo\n",
    "\n",
    "hidden_size = 8\n",
    "model = nn.RNN(embedding_size, hidden_size, batch_first=False)\n",
    "\n",
    "hidden = None  # nn.RNN irá selecionar h0 como sendo zeros\n",
    "for Xt in X:\n",
    "    outputs, hidden = model(Xt, hidden)\n",
    "\n",
    "print('Tamanho do output final:', outputs.shape)\n",
    "print('Tamanho do hidden final:', hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03679a81-9ad7-4a5e-8373-efcebccbf62c",
   "metadata": {},
   "source": [
    "Como iremos trabalhar com geração de texto, **condicionando** a geração de nomes a partir de um idioma específico, a entrada do nosso modelo no tempo $t$ será a concatenação do *embedding* do *token* atual com o do idioma selecionado. Para isso, teremos dois *embeddings* diferentes: um para a transformação do idioma; e outro para transformação do *token*.\n",
    "\n",
    "Um diagrama do esquema de geração pode ser visto a seguir, note que a nossa arquitetura possui uma modelagem autorregressiva, ou seja, a predição do tempo o $t$ depende das predições do tempo $t-1$, e assim por diante. Nele, o símbolo `&` representa o operador de concatenação das entradas.\n",
    "\n",
    "> A geração da sentença será finalizada quando o modelo prever um *token* de fim de sentença ou quando atingirmos um tamanho predeterminado, forçando assim a adição de um `<eos>` no final da predição.\n",
    "\n",
    "![](../imagens/rnn_autorregressivo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e30b8ceb-fdd1-4c1e-b532-eca48a07befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameGenerationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_languages, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.letter_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.language_embedding = nn.Embedding(num_languages, embedding_size)\n",
    "\n",
    "        self.rnn = nn.RNN(2 * embedding_size, hidden_size, batch_first=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, vocab_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, language, hidden = None):\n",
    "        x = self.letter_embedding(x)\n",
    "        language = self.language_embedding(language)\n",
    "        combined = torch.cat([x, language], dim=-1)  # iremos concatenar ao longo das features\n",
    "\n",
    "        output, hidden = self.rnn(combined, hidden)\n",
    "        output = self.classifier(output)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3959eb62-da34-44f4-adb4-26135c74b472",
   "metadata": {},
   "source": [
    "Exemplo da utilização da rede para o tempo $t = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d276d5d1-60fa-4887-932d-62d8c406d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saída do modelo: tensor([[-4.8061, -4.3098, -4.3283, -4.4460, -4.8979, -4.6560, -4.3308, -4.6303,\n",
      "         -3.8544, -4.6284, -4.1091, -4.3597, -4.5995, -4.1687, -4.7208, -4.7020,\n",
      "         -4.4648, -4.1311, -4.6926, -4.8525, -4.2456, -4.9046, -4.3075, -4.5524,\n",
      "         -3.9136, -4.3583, -4.6333, -3.9953, -4.6038, -4.4732, -4.9516, -4.2999,\n",
      "         -4.6537, -4.6251, -4.5576, -4.1666, -4.4304, -4.1446, -4.0455, -4.1321,\n",
      "         -4.9120, -4.2889, -4.4654, -4.9966, -4.4777, -4.3767, -4.8496, -4.5319,\n",
      "         -4.4120, -4.8735, -4.5910, -4.3968, -4.3801, -4.3937, -4.6852, -4.3528,\n",
      "         -4.0991, -4.3277, -4.6708, -4.0686, -4.3730, -4.2455, -4.5019, -4.0963,\n",
      "         -4.3959, -4.6232, -4.5748, -4.8507, -4.7975, -4.6344, -4.5530, -4.4877,\n",
      "         -4.4590, -4.1904, -4.2463, -4.2500, -4.2321, -5.0425, -4.3649, -4.1137,\n",
      "         -4.3435, -4.6682, -4.4198, -4.7954]], grad_fn=<LogSoftmaxBackward0>)\n",
      "\n",
      "Estado oculto: tensor([[-0.0822, -0.2311, -0.3425,  0.5709,  0.1809,  0.5220, -0.2139, -0.3118,\n",
      "         -0.3359,  0.4224,  0.6159, -0.8886,  0.0239,  0.1703, -0.4020, -0.6861,\n",
      "          0.4761, -0.3487, -0.1067,  0.1476, -0.0977,  0.6330, -0.4678, -0.4296,\n",
      "         -0.1296,  0.0615,  0.5770,  0.4105, -0.7591, -0.0625,  0.7396, -0.1347]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "Próximo token previsto: N\n"
     ]
    }
   ],
   "source": [
    "token = '<sos>'\n",
    "language = 'Portuguese'\n",
    "\n",
    "token_tensor = token2tensor(token)\n",
    "language_tensor = language2tensor(language)\n",
    "\n",
    "model = NameGenerationModel(len(vocabulary), len(languages), embedding_size=16, hidden_size=32)\n",
    "output, hidden = model(token_tensor, language_tensor)\n",
    "\n",
    "print('Saída do modelo:', output)\n",
    "print('\\nEstado oculto:', hidden)\n",
    "\n",
    "next_token = output.argmax()\n",
    "print('\\nPróximo token previsto:', index2token[next_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22bc3e3-237a-4bfd-b882-c1ce63eefe94",
   "metadata": {},
   "source": [
    "## Treinamento do modelo\n",
    "\n",
    "Para o treinamento do modelo generativo, iremos introduzir um novo conceito conhecido como *teacher forcing*, uma estratégia de treinamento de geração de sequências para acelerar a convergência do modelo e garantir estabilidade ao longo do treinamento. A estratégia consiste em utilizar como entrada para o tempo $t$ o *token* real do tempo $t-1$ ao invés da predição do modelo.\n",
    "\n",
    "À primeira vista, essa estratégia pode parecer \"roubada\". Porém, temos que lembrar que o predição do modelo no tempo $t$ é dada pelo *ground truth* do tempo $t-1$ e o estado oculto do modelo que foi atualizado utilizando os *ground truths* do tempo $1$ até $t-2$. Ou seja, o modelo ainda está aprendendo os padrões linguísticos da sentença que lhe foi apresentada, mesmo não usando as suas próprias predições. Uma analogia interessante que podemos fazer com *teacher forcing* é quando vamos fazer uma prova onde a questão **d)** depende da resposta correta da questão **c)** e assim por diante. Se errarmos a questão **a)**, estaremos correndo o risco de errar todas as questões seguintes, mesmo realizando os cálculos corretos. Porém, se no começo de cada questão nós começarmos com a resposta correta da anterior, conseguimos acertar algumas coisas.\n",
    "\n",
    "> **Importante:** Durante a geração de sentenças fora do treino, utilizaremos as predições do modelo ao invés do *ground truth* da sentença, uma vez que tal informação não está disponível. Sendo assim, é esperado observar uma discrepância da performance do modelo. Isso é conhecido na literatura como *exposure bias*, e uma forma simples de mitigar esse efeito é de utilizar *teacher forcing* de maneira probabilística, reduzindo a probabilidade de uso a medida que o modelo prevê novos *tokens* da sentença. Outra estratégia interessante pode ser lida no seguinte [artigo](https://arxiv.org/pdf/2103.11603.pdf), onde os autores propõem fornecer para o modelo um conjunto de palavras similares à do passo $t-1$ ao invés de um único *ground truth*.\n",
    "\n",
    "- Apenas para relembrar: Como o nosso modelo recorrente possui como última camada uma `LogSoftmax`, iremos utilizar a função de perda `nn.NLLLoss` (*negative log-likelihood*), que espera log-probabilidades como saída da rede. Um fato curioso é que `nn.CrossEntropyLoss = LogSoftmax + nn.NLLLoss`, segundo a própria [documentação](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) da `nn.CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75adf7a2-7798-46fa-9861-ec84388ec1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NameGenerationModel(len(vocabulary), len(languages), embedding_size=64, hidden_size=128)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "loss_interval = 1000  # intervalo para salvar a loss média\n",
    "print_interval = 5000  # intervalo para exibir performance da rede\n",
    "num_iterations = 100000\n",
    "\n",
    "# Variáveis para manter o valor da loss ao longo das épocas\n",
    "all_losses = []\n",
    "average_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eece3d-8461-413e-ac80-27b5461a714f",
   "metadata": {},
   "source": [
    "Algo a se notar aqui é que ao invés de usarmos a última saída da rede para calcular a função de custo, como no caso da classificação de nomes, estaremos realizando uma predição a cada instante de tempo. Logo, precisaremos calcular a função de custo para cada predição.\n",
    "\n",
    "Para lidar com isso, o `autograd` do PyTorch permite você apenas somar todos os valores das funções de custo em cada instante de tempo e depois chamar `.backward()` no fim.\n",
    "\n",
    "> **Importante:** A verificação da qualidade da sentença gerada será utilizada através do [BLEU Score](https://cloud.google.com/translate/automl/docs/evaluate?hl=pt-br#bleu), uma métrica usada para avaliação automática do texto traduzido por máquina. Note que estaremos utilizando essa pontuação fora do domínio na qual ela foi proposta, porém, ainda podemos obter *insights* interessantes ao utilizar essa pontuação, principalmente para verificar o quão discrepante a nossa geração está do *corpus* fornecido, uma vez que o *BLEU Score* verifica a similaridade da sentença gerada e do *corpus* através de um modelo de *n-grama*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c35d9fc1-4995-463e-8684-789e43032da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c4219fef7c497c98397a251ead6119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/ej0f/.local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/u/ej0f/.local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [5000/100000] => loss: 17.67373, language: Spanish, predicted name: Rrrrr, BLEU score: 0.00000\n",
      "Iter [10000/100000] => loss: 15.71425, language: English, predicted name: Eiann, BLEU score: 0.00000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/pgeoprj/godeep/ej0f/CIAG/ciag2024/sequencias/name_generation.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/pgeoprj/godeep/ej0f/CIAG/ciag2024/sequencias/name_generation.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         predicted_tokens\u001b[39m.\u001b[39mappend(token)\n\u001b[1;32m     <a href='vscode-notebook-cell:/pgeoprj/godeep/ej0f/CIAG/ciag2024/sequencias/name_generation.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/pgeoprj/godeep/ej0f/CIAG/ciag2024/sequencias/name_generation.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/pgeoprj/godeep/ej0f/CIAG/ciag2024/sequencias/name_generation.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/pgeoprj/godeep/ej0f/CIAG/ciag2024/sequencias/name_generation.ipynb#X43sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m average_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m  loss\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m (\u001b[39mlen\u001b[39m(name_tensor) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:491\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    483\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    484\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    490\u001b[0m     )\n\u001b[0;32m--> 491\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    492\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    493\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    206\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in tqdm(range(1, num_iterations + 1)):\n",
    "    name, language, name_tensor, language_tensor = get_random_pair()\n",
    "    \n",
    "    name_tensor = name_tensor.to(device)\n",
    "    language_tensor = language_tensor.to(device)\n",
    "\n",
    "    loss = 0\n",
    "    hidden = None\n",
    "    predicted_tokens = []\n",
    "\n",
    "    # Iremos iterar sobre todos os tokens até o penúltimo\n",
    "    for idx in range(len(name_tensor) - 1):\n",
    "        output, hidden = model.forward(name_tensor[idx], language_tensor, hidden)  # note que estamos sempre fazendo teacher forcing\n",
    "        loss += criterion(output, name_tensor[idx+1])\n",
    "\n",
    "        # Salvando o nome predito pelo modelo\n",
    "        token = index2token[output.argmax()]\n",
    "        if token != '<eos>':\n",
    "            predicted_tokens.append(token)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    average_loss +=  loss.item() / (len(name_tensor) - 1)\n",
    "    \n",
    "    if iter % print_interval == 0:\n",
    "        predicted_name = ''.join(predicted_tokens)  # convertendo de lista de caracteres para string\n",
    "        print(f'Iter [{iter}/{num_iterations}] => loss: {loss:.5f}, ' \\\n",
    "              f'language: {language}, predicted name: {predicted_name}, ' \\\n",
    "              f'BLEU score: {sentence_bleu(language_names[language], predicted_tokens):.5f}')\n",
    "\n",
    "    if iter % loss_interval == 0:\n",
    "        all_losses.append(average_loss / loss_interval)\n",
    "        average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde12879-2212-43cd-be8b-79b2056ce5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Evolução da função de perda média ao longo das iterações')\n",
    "plt.xlabel('Intervalo de Salvamento')\n",
    "plt.ylabel('Valor médio')\n",
    "    \n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc696a-94b8-482d-b85c-e28a3abc0128",
   "metadata": {},
   "source": [
    "## Avaliando os resultados obtidos\n",
    "\n",
    "Para verificar o desempenho da rede em diferentes línguas, criaremos uma matriz de confusão, indicando para cada idioma real (linhas) qual idioma a rede adivinha (colunas). Para calcular a matriz de confusão, uma quantia grande de amostras, definida pela variável `num_confusion_samples`, serão processadas pela rede em modo de avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1d04e-e4b4-45ca-be98-75ef27394cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_confusion_samples = 10000\n",
    "confusion_matrix = torch.zeros(len(languages), len(languages))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(num_confusion_samples)):\n",
    "        name, language, name_tensor, language_tensor = get_random_pair()\n",
    "\n",
    "        name_tensor = name_tensor.to(device)\n",
    "        language_tensor = language_tensor.to(device)\n",
    "\n",
    "        output = model(name_tensor)\n",
    "        guess = get_language_from_output(output)\n",
    "\n",
    "        guess_idx = languages.index(guess)\n",
    "        language_idx = languages.index(language)\n",
    "\n",
    "        confusion_matrix[language_idx, guess_idx] += 1\n",
    "\n",
    "# Normalizando as linhas da matriz de confusão\n",
    "for i in range(len(languages)):\n",
    "    confusion_matrix[i] /= confusion_matrix[i].sum()\n",
    "\n",
    "# Configurando o plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "img = ax.matshow(confusion_matrix.numpy())\n",
    "fig.colorbar(img)\n",
    "\n",
    "ax.set_xticks(range(len(languages)), languages, rotation=90)\n",
    "ax.set_yticks(range(len(languages)), languages)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabfd144-d3d6-4534-ab96-6b1a6ea644de",
   "metadata": {},
   "source": [
    "## Verificando as top 3 predições da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11a96e-9b04-4a02-a45b-d3ad69925856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(name):\n",
    "    print(f'\\n> {name}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        name_tensor = name2tensor(name).to(device)\n",
    "        output = model(name_tensor)\n",
    "\n",
    "        vals, idxs = torch.topk(output[0], k=3, dim=-1)\n",
    "        for val, idx in zip(vals, idxs):\n",
    "            prob = torch.e ** val  # probabilidade = exp(log-probabilidade)\n",
    "            print(f'({prob:.2f}) {languages[idx]}')\n",
    "            \n",
    "predict('Dovesky')\n",
    "predict('Jackson')\n",
    "predict('Satoshi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ej0f",
   "language": "python",
   "name": "ej0f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
