{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6f71a2-22b9-4495-87a1-c90c3fa89c64",
   "metadata": {},
   "source": [
    "# Sequências - Aula Prática 01/04\n",
    "<!-- ## Analogias a partir de representações distribuídas e Word2Vec\n",
    "\n",
    "Neste notebook iremos trabalhar um pouco com a parte introdutória de modelos de linguagem, sendo dividido em duas partes:\n",
    "1. A primeira parte será relacionada com analogias a partir de **representações distribuídas**.\n",
    "2. Na segunda parte iremos implementar um modelo `Word2Vec`, mais especificamente um Constant Bag-of-Words (CBOW).\n",
    "\n",
    "> O notebook será dividido em duas partes. A primeira será relacionada com analogias a partir das **representações distribuídas** gerada pelo modelo de *embedding*. Já a segunda  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81e671-4806-46c1-bf82-a7ae9336075e",
   "metadata": {},
   "source": [
    "- Se estiver utilizando a Tatu, execute a próxima célula. Caso contrário, ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3f3a1-641a-47ef-9ff2-395b7a5e67f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chave:  ej0f\n",
      "Senha: ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variáveis de ambiente http_proxy e https_proxy configuradas!\n"
     ]
    }
   ],
   "source": [
    "%load_ext nbproxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5b060-73b1-4e0d-9089-c8fb743e1d66",
   "metadata": {},
   "source": [
    "## Importação de pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165d22a-0759-464d-bbfc-da5536140c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0626f6-ffa7-4ba4-a1a4-1c025840bfab",
   "metadata": {},
   "source": [
    "- Definição de uma pasta temporária para salvar dados gerados pelo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c019c43-bab7-4a70-857a-4307c13bd3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using temporary directory: /tmp/tmpg1l_bx7s\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "tmpdir = tempfile.TemporaryDirectory()\n",
    "print('Using temporary directory:', tmpdir.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd96627-d0b2-4488-b013-7debf3f8690a",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "A representação de palavras ou termos por *embeddings* é um dos conceitos mais fundamentais de Deep Learning em Processamento de Linguagem Naturais. O *word2vec*, proposto por Tomas Mikolov et al. no Google em 2013, foi um dos modelos iniciais utilizados para se aprender esse tipo de representação. Apesar de já ser considerado antigo, os conceitos desenvolvidos nas primeiras publicações ainda são úteis para o desenvolvimento de modelos mais avançados.\n",
    "\n",
    "A ideia central do *word2vec* é a de que o significado de uma palavra está diretamente relacionado às palavras ao redor da mesma, ou seja, seu contexto. Por exemplo, podemos imaginar que as palavras que se encaixem em uma frase do tipo \"hoje eu comi ___ no café da manhã\" tenham uma certa proximidade de significado em alguns aspectos, e portanto possuam um grau de similaridade entre seus embeddings.\n",
    "\n",
    "O artigo original propõe duas arquiteturas distintas para isso: **CBOW** (Continuous Bag-of-Words) e **Skip-Gram**.\n",
    "\n",
    "<img width=600 src=\"https://www.researchgate.net/profile/Daniel-Braun-6/publication/326588219/figure/fig1/AS:652185784295425@1532504616288/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations.png\">\n",
    "\n",
    "A arquitetura CBOW recebe como entrada da rede o conjunto de termos que formam o contexto e tenta prever o termo central da sequência. Já a arquitetura Skip-Gram tenta prever os termos do contexto com base na palavra central. Apesar dessa diferença, o objetivo em ambos os casos é gerar os embeddings dos termos. Portanto o que importa para nós no final são as representações aprendidas pelo modelo durante o treinamento e armazenados como parâmetros da rede.\n",
    "\n",
    "Os detalhes da proposta inicial do algoritmo Word2Vec podem ser estudados pelo artigo no [link](https://research.google/pubs/pub41224/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7423a5f-6320-4552-a1ef-b3dbf263f9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
